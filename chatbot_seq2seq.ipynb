{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 및 라이브러리를 임포트\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시각화 함수\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 시드값\n",
    "SEED_NUM = 42\n",
    "tf.random.set_seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터의 경로\n",
    "DATA_IN_PATH = './data/'\n",
    "DATA_OUT_PATH = './check_point/'\n",
    "TRAIN_INPUTS = 'train_inputs.npy'\n",
    "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
    "TRAIN_TARGETS = 'train_targets.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "# 미리 전처리된 학습에 필요한 데이터와 설정값을 불러옴\n",
    "index_inputs = np.load(open(DATA_IN_PATH + TRAIN_INPUTS, 'rb'))\n",
    "index_outputs = np.load(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'rb'))\n",
    "index_targets = np.load(open(DATA_IN_PATH + TRAIN_TARGETS , 'rb'))\n",
    "prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 하이퍼파라미터\n",
    "MODEL_NAME = 'seq2seq_kr'\n",
    "BATCH_SIZE = 16\n",
    "MAX_SEQUENCE = 25\n",
    "EPOCHS = 1\n",
    "ENC_UNITS = 128\n",
    "DEC_UNITS = 128\n",
    "EMBEDDING_DIM = 64\n",
    "VALID_SPLIT = 0.1\n",
    "\n",
    "char2idx = prepro_configs['char2idx']\n",
    "idx2char = prepro_configs['idx2char']\n",
    "std_index = prepro_configs['std_symbol']\n",
    "end_index = prepro_configs['end_symbol']\n",
    "vocab_size = prepro_configs['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 단어를 emdding_dim차원의 임베딩 벡터로 만든다\n",
    "        self.embedding = Embedding(self.vocab_size, self.embedding_dim)\n",
    "        # 재귀 신경망(GRU)\n",
    "        self.gru = GRU(self.enc_units, return_sequences=True,\n",
    "                      return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self, inp):\n",
    "        return tf.zeros((tf.shape(inp)[0], self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 가중치를 이용해 디코더에 전달할 새로운 GRU의 결괏값을 만든다\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # 출력 벡터의 크기(units)를 인자로 받아 W1,W2,V의 완전 연결 계층(FC)를 만든다\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V  = Dense(1)\n",
    "        \n",
    "    # query: Encoder GRU의 은닉층의 상태 값, values: Encoder GRU의 결괏값 \n",
    "    def call(self, query, values):\n",
    "        # query를 W2에 행렬곱을 할 수 있는 형태로 만든다\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        # W1과 W2의 결괏값의 요소를 각각 더하고 활성함수를 통과한 값을 V에 행렬곱하여 1차원의 벡터값을 만든다.\n",
    "        score = self.V(tf.nn.tanh(\n",
    "                        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # 이 값을 softmax에 넣어 어텐션 가중치를 얻는다\n",
    "        # attention_weights는 모델이 중요하다고 판단하는 값은 1에 가까워지고 중요하지 않다고 판단할 수록 0에 가까워지는 값\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # 위에서 구한 어텐션 가중치에 GRU 결괏값인 values를 행렬 곱을 하면,\n",
    "        # 1에 가까운 값에 위치한 value값(모델이 중요하다고 판단하는 값)은 커지고, 0에 가까운 값에 위치한 value값(모델이 중요치 않다고 판단하는 값)은 작아진다\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = GRU(self.dec_units, return_sequences=True,\n",
    "                      return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = Dense(self.vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    # hidden: Enocder의 은닉 상태 값, enc_output: Encoder의 결괏값\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # 임베딩 벡터 x와 어텐션 가중치를 곱한 context_vector를 결합해 다시 x를 구성\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # GRU를 통과시한 결괏값을 완전 연결 계층(FC)에 연결해 사전 크기의 벡터 x를 만든다\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수, 정확도 측정 함수\n",
    "# 최적화 객체\n",
    "optimizer = 'adam'\n",
    "# 손실 값을 측정하기 위한 객체\n",
    "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "# 정확도 측정을 위한 객체\n",
    "train_accuracy = SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "# 손실 함수\n",
    "def loss(real, pred):\n",
    "    # mask= real값 중 0인 값(<PAD>)를 제외\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# 정확도 측정 함수\n",
    "def accuracy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "    \n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq 모델\n",
    "class seq2seq(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, dec_units, batch_size, end_token_idx=2):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.end_token_idx = end_token_idx\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_size)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        inp, tar = x\n",
    "        \n",
    "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
    "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        # predict_tokens= 시퀀스마다 나온 결괏값, 손실 또는 정확도를 계산하는 용도로 사용\n",
    "        predict_tokens = list()\n",
    "        for t in range(0, tar.shape[1]):\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims(tar[:, t], 1), tf.float32)\n",
    "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))\n",
    "            \n",
    "        return tf.stack(predict_tokens, axis=1)\n",
    "    \n",
    "    # 사용자의 입력에 대한 결과를 확인하기 위해, 테스트 목적의 프로토콜 함수\n",
    "    def inference(self, x):\n",
    "        inp = x\n",
    "        \n",
    "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
    "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        dec_input = tf.expand_dims([char2idx[std_index]], 1)\n",
    "        \n",
    "        predict_tokens = list()\n",
    "        for t in range(0, MAX_SEQUENCE):\n",
    "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "            predict_token = tf.argmax([predictions[0]])\n",
    "            \n",
    "            # <END> 토큰을 만나면 정지, 하나의 배치에 대해서만 동작\n",
    "            if predict_token == self.end_token_idx:\n",
    "                break\n",
    "            \n",
    "            predict_tokens.append(predict_token)\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token], 0), tf.float32)\n",
    "            \n",
    "        return tf.stack(predict_tokens, axis=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq 모델 생성\n",
    "model = seq2seq(vocab_size, EMBEDDING_DIM, ENC_UNITS, DEC_UNITS,\n",
    "               BATCH_SIZE, char2idx[end_index])\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq_kr Folder already exists!\n"
     ]
    }
   ],
   "source": [
    "# 모델 체크포인트를 저장하고 과적합을 방지하기 위해 조기 종료를 정의한 함수를 선언\n",
    "PATH = DATA_OUT_PATH + MODEL_NAME\n",
    "\n",
    "if os.path.isdir(PATH):\n",
    "    print(MODEL_NAME + ' Folder already exists!')\n",
    "    \n",
    "if not(os.path.isdir(PATH)):\n",
    "    os.makedirs(os.path.join(PATH))\n",
    "    print(MODEL_NAME + ' Folder is created!')\n",
    "    \n",
    "checkpoint_path = DATA_OUT_PATH + MODEL_NAME + '/weights.h5'\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665/665 [==============================] - 157s 201ms/step - loss: 0.8331 - accuracy: 0.8812 - val_loss: 0.8953 - val_accuracy: 0.8853\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.88526, saving model to ./check_point/seq2seq_kr/weights.h5\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "# 현재는 모델이 학습되는지를 빠른 시간 내 검증하기 위해 하이퍼파라미터 값을 조정하여 실행\n",
    "# 1epoch당 2:30 소요, 30epoch 시 accuracy: 0.8985\n",
    "history = model.fit([index_inputs, index_outputs], index_targets,\n",
    "                   batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=VALID_SPLIT,\n",
    "                   callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch에 따른 accuracy 그래프\n",
    "# plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch에 따른 loss 그래프\n",
    "# plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델을 로딩\n",
    "SAVE_FILE_NAME = 'weights.h5'\n",
    "model.load_weights(os.path.join(DATA_OUT_PATH, MODEL_NAME, SAVE_FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ba5de8c80a71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_index_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpredict_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredict_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 평소에 필요했던 게 좋을 것 같아요\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-23df84c94e62>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# <END> 토큰을 만나면 정지, 하나의 배치에 대해서만 동작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mpredict_token\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_token_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m   \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# 챗봇 예시 확인\n",
    "question = '남자친구 승진 선물로 뭐가 좋을까?'\n",
    "\n",
    "test_index_inputs, _ = enc_processing([question], char2idx)\n",
    "predict_tokens = model.inference(test_index_inputs)\n",
    "\n",
    "print(' '.join([char2word[w] for w in predict_tokens])) # 평소에 필요했던 게 좋을 것 같아요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
